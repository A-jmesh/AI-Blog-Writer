╒══════════════════════╤══════════════════════╤══════════════════╤══════════════════════════════════════════════════════════════╕
│ URL                  │ Title                │ Published Date   │ Summary                                                      │
╞══════════════════════╪══════════════════════╪══════════════════╪══════════════════════════════════════════════════════════════╡
│ https://www.analytic │ Using Llamafiles to  │ 2024-01-18       │ The article discusses Llamafiles, which simplify the process │
│ svidhya.com/blog/202 │ Simplify LLM         │                  │ of running Large Language Models (LLMs) on consumer          │
│ 4/01/using-          │ Execution            │                  │ hardware. Traditionally, running LLMs involved downloading   │
│ llamafiles-to-       │                      │                  │ third-party software, creating Python environments, and      │
│ simplify-llm-        │                      │                  │ writing code. Llamafiles address these challenges by         │
│ execution/           │                      │                  │ enabling users to download and run LLMs as single-file       │
│                      │                      │                  │ executables. Additionally, the article explains the concept  │
│                      │                      │                  │ of Llamafiles, including its benefits and limitations, as    │
│                      │                      │                  │ well as how to create Llamafiles from quantized LLMs.        │
├──────────────────────┼──────────────────────┼──────────────────┼──────────────────────────────────────────────────────────────┤
│ https://levelup.gitc │ Live Indexing for    │ 2024-01-08       │ - Most AI systems, including LLMs, struggle to process and   │
│ onnected.com/live-   │ RAG: A Guide For     │                  │ answer questions from PDFs due to their complex information. │
│ indexing-for-rag-a-  │ Real-Time Indexing   │                  │ - RAG frameworks and Large Language Models (LLMs) have       │
│ guide-for-real-time- │ Using LlamaIndex and │                  │ enabled the creation of full-stack applications for          │
│ indexing-using-      │ AWS                  │                  │ interacting with PDFs. - LlamaIndex is provided as an        │
│ llamaindex-and-aws-5 │                      │                  │ example of a RAG framework that allows users to create chat  │
│ 1353083ace4?gi=472c9 │                      │                  │ applications for interacting with PDFs with just a few lines │
│ 89ddb71&source=rss   │                      │                  │ of code. - The article also discusses additional challenges  │
│ ----5517fd7b58a6---4 │                      │                  │ for AI engineers in creating enterprise-grade RAG            │
│                      │                      │                  │ applications such as re-indexing and live updates.           │
├──────────────────────┼──────────────────────┼──────────────────┼──────────────────────────────────────────────────────────────┤
│ https://tech.dentsus │ LlamaIndexを使ってロ │ 2024-01-22       │ This webpage discusses how to implement Retrieval-Augmented  │
│ oken.com/entry/2024/ │ ーカル環境でRAGを実  │                  │ Generation (RAG) using the LlamaIndex library in a local     │
│ 01/22/LlamaIndex%E3% │ 行する方法           │                  │ environment. The goal is to leverage Large Language Models   │
│ 82%92%E4%BD%BF%E3%81 │                      │                  │ (LLMs) like ChatGPT while addressing limitations such as     │
│ %A3%E3%81%A6%E3%83%A │                      │                  │ data confidentiality and restricted internet access. The     │
│ D%E3%83%BC%E3%82%AB% │                      │                  │ article highlights the benefits of using a local setup for   │
│ E3%83%AB%E7%92%B0%E5 │                      │                  │ LLM applications and explains why the LlamaIndex framework   │
│ %A2%83%E3%81%A7RAG%E │                      │                  │ is suitable for this purpose. The author provides detailed   │
│ 3%82%92%E5%AE%9F%E8% │                      │                  │ instructions on setting up the environment, including        │
│ A1%8C%E3%81%99%E3%82 │                      │                  │ installing necessary software and configuring a development  │
│ %8B%E6%96%B9%E6%B3%9 │                      │                  │ container using Docker. Furthermore, the article guides      │
│ 5                    │                      │                  │ readers through the process of loading data, initializing    │
│                      │                      │                  │ LLM and embedding models, and implementing RAG using Python  │
│                      │                      │                  │ code. It also includes a sample implementation of a chat     │
│                      │                      │                  │ system that leverages RAG to answer questions based on a     │
│                      │                      │                  │ provided text document. The author discusses the challenges  │
│                      │                      │                  │ faced during implementation and suggests potential           │
│                      │                      │                  │ improvements, such as optimizing performance by reducing     │
│                      │                      │                  │ context information and leveraging more powerful hardware.   │
│                      │                      │                  │ The article concludes by encouraging readers to experiment   │
│                      │                      │                  │ with RAG and emphasizing the potential of this technology to │
│                      │                      │                  │ create useful applications.                                  │
├──────────────────────┼──────────────────────┼──────────────────┼──────────────────────────────────────────────────────────────┤
│ https://dev.to/lgram │ Create Your Own      │ 2024-01-13       │ This blog post provides a detailed guide on how to build a   │
│ mel/create-your-own- │ Local Chatbot with   │                  │ local chatbot using several technologies. Here's a summary:  │
│ local-chatbot-with-  │ Next.js, Llama.cpp,  │                  │ Objective of the blog post: - Build a chatbot that runs on   │
│ nextjs-llamacpp-and- │ and ModelFusion      │                  │ your computer using Next.js, Llama.cpp, and ModelFusion. -   │
│ modelfusion-461j     │                      │                  │ Use the OpenHermes 2.5 Mistral LLM (large language model)    │
│                      │                      │                  │ for natural language interaction. - Employ the Vercel AI SDK │
│                      │                      │                  │ for stream forwarding and rendering. - Integrate the         │
│                      │                      │                  │ Llama.cpp language model with the Vercel AI SDK through      │
│                      │                      │                  │ ModelFusion.  Necessary Steps: 1. Setup Llama.cpp:    a)     │
│                      │                      │                  │ Clone the repository.    b) Build Llama.cpp: Linux/Mac users │
│                      │                      │                  │ can run "make", Windows users can follow the instructions    │
│                      │                      │                  │ provided.    c) Download the OpenHermes 2.5 Mistral GGUF     │
│                      │                      │                  │ model from HuggingFace and move it into the Llama.cpp        │
│                      │                      │                  │ repository's "models/" directory.    d) Start the Llama.cpp  │
│                      │                      │                  │ server to enable the integration of the model into the       │
│                      │                      │                  │ chatbot.  2. Create a Next.js Project:    a) Create a new    │
│                      │                      │                  │ Next.js project using "npx create-next-app@latest llamacpp-  │
│                      │                      │                  │ nextjs-chatbot".    b) Configure the project with preferred  │
│                      │                      │                  │ settings, including TypeScript, ESLint, Tailwind CSS, and    │
│                      │                      │                  │ App Router.  3. Install Required Libraries:    a) Install    │
│                      │                      │                  │ libraries such as Vercel AI SDK, ModelFusion, and            │
│                      │                      │                  │ ModelFusion Vercel AI SDK Integration.  4. Creating an API   │
│                      │                      │                  │ Route for the Chatbot:    a) In the 'api/chat/' directory,   │
│                      │                      │                  │ create 'route.ts' for handling chat interactions.    b)      │
│                      │                      │                  │ Import relevant modules and initialize a ModelFusion text    │
│                      │                      │                  │ generation model.    c) Send the API request, process the    │
│                      │                      │                  │ response, and generate a streaming response using            │
│                      │                      │                  │ ModelFusion to access the Llama.cpp chat API.  5. Adding the │
│                      │                      │                  │ Chat Interface:    a) Establish a chat page, 'page.tsx' to   │
│                      │                      │                  │ display the chatbot and use the 'useChat' hook from the      │
│                      │                      │                  │ Vercel AI SDK.    b) Clean up the global styles for better   │
│                      │                      │                  │ UI presentation.  6. Running the Chatbot Application:    a)  │
│                      │                      │                  │ Launch the development server with "npm run dev".    b) In a │
│                      │                      │                  │ browser, navigate to "http://localhost:3000" to interact     │
│                      │                      │                  │ with the chatbot.  Conclusion: The tutorial provides a step- │
│                      │                      │                  │ by-step guide to set up a local chatbot, enabling users to   │
│                      │                      │                  │ explore AI and natural language processing.                  │
├──────────────────────┼──────────────────────┼──────────────────┼──────────────────────────────────────────────────────────────┤
│ https://akash-mathur │ Advanced RAG: Query  │ 2024-01-18       │ Welcome to the Advanced RAG Learning Series. This article    │
│ .medium.com/advanced │ Augmentation for     │                  │ series explores advanced techniques to heighten              │
│ -rag-query-          │ Next-Level Search    │                  │ understanding and expertise in Retriever-Augmented           │
│ augmentation-for-    │ using LlamaIndex🦙   │                  │ Generation (RAG) applications.  Key concepts covered include │
│ next-level-search-   │                      │                  │ optimizing retrieval with extra context and metadata,        │
│ using-llamaindex-d36 │                      │                  │ improving retrieval efficiency via rerankers, and enhancing  │
│ 2fed7ecc3            │                      │                  │ query augmentation.  The focus is on query transformations   │
│                      │                      │                  │ which bridge user prompts and relevant information in vast   │
│                      │                      │                  │ databases, particularly to address the challenge of          │
│                      │                      │                  │ retrieval misalignment.  Five powerful query transformation  │
│                      │                      │                  │ techniques are explored, addressing the need to adapt to     │
│                      │                      │                  │ LLMs' comprehension and generation capabilities. The         │
│                      │                      │                  │ techniques explored are:  - Hypothetical Document Embeddings │
│                      │                      │                  │ (HyDE), which creates a hypothetical answer document and     │
│                      │                      │                  │ encodes it to retrieve relevant documents. - Sub-Question    │
│                      │                      │                  │ Query Engine, which decomposes complex queries into sub-     │
│                      │                      │                  │ questions and retrieves results from dedicated data sources. │
│                      │                      │                  │ - Router Query Engine, which selects the most appropriate    │
│                      │                      │                  │ query engine based on user queries and metadata. - Single-   │
│                      │                      │                  │ Step Query Decomposition, which breaks down complex          │
│                      │                      │                  │ questions into simpler sub-queries for focused information   │
│                      │                      │                  │ extraction. - Multi-Step Query Decomposition, which employs  │
│                      │                      │                  │ a self-ask method to iteratively explore knowledge and       │
│                      │                      │                  │ uncover hidden connections among facts.  The article         │
│                      │                      │                  │ provides code examples and GitHub links to assist in         │
│                      │                      │                  │ practical implementation. It also highlights the ongoing     │
│                      │                      │                  │ developments and potential future directions in query        │
│                      │                      │                  │ augmentation research.                                       │
╘══════════════════════╧══════════════════════╧══════════════════╧══════════════════════════════════════════════════════════════╛


